You are building the `parse_and_chunk()` function for a regulatory data intelligence platform.
This is Node 1 of a LangGraph pipeline that extracts regulatory requirements from GRC tool
exports and regulatory documents used in retail banking (FDIC Part 370 compliance).

## WHAT THIS FUNCTION DOES

Reads a raw file (.docx, .xlsx, .csv), strips formatting artifacts, detects structure,
and splits the content into clean text blocks (ContentChunk[]).

This is a DETERMINISTIC FUNCTION — no LLM calls. Pure Python parsing.
It runs as a LangGraph node. Its output is stored in pipeline state and consumed by
two downstream nodes:
  1. Schema Discovery Agent (reads chunks to infer document structure)
  2. Requirement Atomizer Agent (reads chunks + resolved schema to extract requirements)

## TECHNICAL STACK

- Python 3.11+
- python-docx (for .docx files)
- openpyxl (for .xlsx files)
- csv stdlib (for .csv files)
- pydantic v2 (for all models)
- structlog (for logging)
- chardet (for encoding detection on csv)
- hashlib (for deterministic chunk IDs)

## DIRECTORY STRUCTURE

Place files here:
egulatory_data_intelligence/
├── agent1/
│   ├── models/
│   │   ├── input.py          # AgentInput model (create this)
│   │   ├── chunks.py         # ContentChunk, PreprocessorOutput (create this)
│   ├── nodes/
│   │   ├── preprocessor.py   # THE MAIN FILE — parse_and_chunk() lives here
│   ├── parsers/
│   │   ├── init.py
│   │   ├── docx_parser.py    # .docx specific parsing logic
│   │   ├── xlsx_parser.py    # .xlsx specific parsing logic
│   │   ├── csv_parser.py     # .csv specific parsing logic


## DATA MODELS TO IMPLEMENT

### AgentInput (in agent1/models/input.py)
```python
from pathlib import Path
from typing import Literal
from pydantic import BaseModel

class AgentInput(BaseModel):
    file_path: Path
    file_type: Literal["docx", "xlsx", "csv"] | None = None  # auto-detect from extension if None
    institution_name: str | None = None
    source_tool: str | None = None  # "Archer", "ServiceNow", "MetricStream", etc.
```

### ContentChunk (in agent1/models/chunks.py)
```python
from pydantic import BaseModel, Field
from typing import Literal
import hashlib

class ContentChunk(BaseModel):
    chunk_id: str                    # deterministic hash of content + position
    chunk_type: Literal["table", "prose", "heading", "list", "mixed"]
    content_text: str                # cleaned text representation
    table_data: list[list[str]] | None = None  # if table: rows × cols as strings
    row_count: int | None = None
    col_count: int | None = None
    source_location: str             # "page 3", "table 7", "rows 45-120", "sheet:Sheet1"
    parent_heading: str | None = None  # nearest heading above this chunk
    char_count: int = 0

class PreprocessorOutput(BaseModel):
    file_path: str
    file_type: str
    total_chunks: int
    chunks: list[ContentChunk]
    document_stats: dict  # table_count, prose_sections, total_chars, etc.
```

## FUNCTION SIGNATURE
```python
def parse_and_chunk(
    file_path: Path,
    file_type: str | None = None,
    max_chunk_chars: int = 3000,
    min_chunk_chars: int = 50,
) -> PreprocessorOutput:
```

## PARSING RULES BY FILE TYPE

### .docx files (GRC exports from Archer, regulatory policy documents)
1. Use python-docx to read paragraphs and tables
2. Track current heading context (last seen heading paragraph)
3. For TABLES:
   - Extract as list[list[str]] preserving row/col structure
   - Each table becomes one chunk with chunk_type="table"
   - Preserve header row separately (first row)
   - Store table_data, row_count, col_count
   - If table exceeds max_chunk_chars, split by rows (keep header row in each split)
4. For PARAGRAPHS:
   - Skip empty paragraphs
   - Detect heading paragraphs (style.name starts with "Heading") → chunk_type="heading"
   - Detect list items (style.name contains "List") → accumulate consecutive list items into one chunk
   - Regular paragraphs → chunk_type="prose"
   - Accumulate consecutive prose paragraphs until max_chunk_chars, then split
5. Strip these artifacts:
   - Page numbers, headers/footers (python-docx doesn't expose these directly, but strip common patterns like "Page X of Y")
   - Watermark text
   - Table of contents entries (detect by style name "TOC")
   - Empty rows in tables

### .xlsx files (GRC library exports, data dictionaries)
1. Use openpyxl in read_only mode for performance
2. Process each sheet separately
3. First non-empty row = header row (detect by checking first 5 rows)
4. Each sheet becomes one or more chunks:
   - If sheet has < max_chunk_chars total: one chunk, chunk_type="table"
   - If larger: split by row groups, keeping header row in each split
5. source_location format: "sheet:{sheet_name}:rows {start}-{end}"
6. Skip completely empty rows
7. Merge cells: use the value from the top-left cell of the merged range

### .csv files
1. Detect encoding using chardet
2. Detect delimiter (csv.Sniffer)
3. First row = header
4. Same splitting logic as xlsx

## CHUNK ID GENERATION

Deterministic hash so the same file always produces the same chunk IDs:
```python
def generate_chunk_id(file_path: str, position: int, content: str) -> str:
    raw = f"{file_path}:{position}:{content[:200]}"
    return f"chunk-{hashlib.sha256(raw.encode()).hexdigest()[:12]}"
```

## ERROR HANDLING

- File not found → raise FileNotFoundError with clear message
- Unsupported file type → raise ValueError("Unsupported file type: {ext}. Supported: .docx, .xlsx, .csv")
- Corrupted/unreadable file → raise FileParseError (custom exception, define in agent1/exceptions.py)
- File parses but produces 0 chunks → raise EmptyDocumentError
- Individual chunk parse failures → log warning, skip chunk, continue

Define custom exceptions:
```python
# agent1/exceptions.py
class FileParseError(Exception):
    """Raised when a file cannot be parsed."""
    pass

class EmptyDocumentError(Exception):
    """Raised when a file produces zero valid chunks."""
    pass
```

## LOGGING

Use structlog throughout:
```python
import structlog
logger = structlog.get_logger(__name__)

# Log at these points:
logger.info("parse_started", file_path=str(file_path), file_type=file_type)
logger.info("parse_completed", total_chunks=len(chunks), total_chars=total_chars)
logger.warning("empty_chunk_skipped", position=pos, reason="below min_chunk_chars")
logger.error("parse_failed", file_path=str(file_path), error=str(e))
```

## DOCUMENT STATS

The `document_stats` dict should include:
```python
{
    "table_count": int,
    "prose_sections": int,
    "heading_count": int,
    "list_sections": int,
    "total_chars": int,
    "empty_chunks_skipped": int,
    "sheets_processed": int,  # xlsx only
}
```

## TESTS TO WRITE

Create tests in `tests/agent1/test_preprocessor.py`:

1. test_docx_with_tables — .docx with 3 tables → produces 3+ chunks, all chunk_type="table"
2. test_docx_with_prose — .docx with paragraphs → prose chunks respect max_chunk_chars
3. test_docx_heading_tracking — headings propagate as parent_heading to subsequent chunks
4. test_xlsx_multisheet — .xlsx with 2 sheets → chunks from both, source_location includes sheet name
5. test_xlsx_header_detection — first non-empty row preserved as header in split chunks
6. test_csv_encoding_detection — UTF-8 and latin-1 csv files both parse correctly
7. test_empty_file — raises EmptyDocumentError
8. test_corrupted_file — raises FileParseError
9. test_unsupported_type — .pdf raises ValueError
10. test_chunk_id_determinism — same file → same chunk IDs every time
11. test_large_table_splitting — table with 500 rows splits into multiple chunks, each has header row

Create minimal test fixtures in `tests/fixtures/`:
- sample_grc_export.docx (3 tables with regulatory control fields)
- sample_data_dict.xlsx (2 sheets, 50 rows each)
- sample_controls.csv (comma-delimited, 20 rows)
- empty.docx (no content)

## IMPORTANT CONSTRAINTS

- NO LLM calls. This is pure Python.
- Chunk IDs MUST be deterministic (same input = same output every time)
- Do NOT normalize or transform content — preserve original text exactly as-is.
  Schema Discovery Agent needs to see the raw field names.
- Table structure (row/col) MUST be preserved in table_data. Schema Discovery
  depends on seeing actual column headers.
- All Pydantic models use v2 syntax (no `Optional[]`, use `X | None` instead)
- Use `from __future__ import annotations` in all files